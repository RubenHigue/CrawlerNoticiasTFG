llm_model: "llama3.2"
judge_models: "mistral, qwen2.5"
news_url: "https://www.libertaddigital.com/defensa/, https://www.eldebate.com/espana/defensa/, https://galaxiamilitar.es, https://www.elmundo.es/espana/ministerio-defensa.html"
cassandra_host: "127.0.0.1"
retrieved_docs: 8
execution_mode: "production"
table_name: "noticias_tabulares"
keyspace: "noticias"
test_data: "csv/evaluacion_tfg_salamandra.csv"
military_prompt: "Eres un asistente militar de defensa para tareas de respuesta a preguntas. Utiliza las siguientes piezas de contexto recuperado para responder a la pregunta. Si no sabes la respuesta, di que no la sabes. Usa un máximo de tres frases y mantén la respuesta concisa."
assistant_prompt: "Eres un asistente para tareas de respuesta a preguntas. Utiliza las siguientes piezas de contexto recuperado para responder a la pregunta. Si no sabes la respuesta, di que no la sabes. Usa un máximo de tres frases y mantén la respuesta concisa."
prompt_generico: 'Rellena el apartado Output de la misma forma que en el ejemplo de Albert Einstein pero para "la answer dada, 1 si tiene relación con el artículo y 0 si no tiene nada de relación. Dado el input, dada la "question" procesa el "context" y saca solo el formato JSON con los siguientes campos: '
prompt_recall: '["statement", el apartado answer del input. "reason", el trozo de contexto por el que se ha guiado la respuesta y "attributed", que sera la puntuación o 1 o 0 en cuestión de la relevancia. Por favor devuelve el Output para que sea igual al siguiente JSON Schema rellenando los datos de statement, reason y attributed: ""classifications": [{"statement": "Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist,widely held to be one of the greatest and most influential scientists of all time.","reason": "The date of birth of Einstein is mentioned clearly in the context.","attributed": 1},]'
prompt_precision: '"reason", la razon por la que se considera que el veredicto es el que es. "veredict", un entero entre 0 y 1 que represente la veracidad que tiene la respuesta dada el contexto. Por favor devuelve el Output para que se adecue al siguiente JSON Schema rellenando los datos reason y veredict: "{"reason": "The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einsteins life and contributions, which are reflected in the answer.", "verdict": 1}'
prompt_eval_manual: "Tienes dos textos: un contexto y una referencia. Queremos evaluar si la referencia está bien representada en el contexto en cuanto a precisión y recall. En una escala de 0.0 a 1.0, ¿qué tan bien se ajusta la referencia al contexto en términos de precisión (0.0 = no es relevante, 1.0 = completamente precisa)? ¿Y qué tan bien se ajusta la referencia al contexto en términos de recall (0.0 = no cubre la referencia, 1.0 = cubre completamente la referencia)? Devuelve dos puntuaciones, una para precisión y otra para recall, separadas por una coma. Siempre incluyendo estas dos como el principio de tu respuesta y de la forma del Ejemplo: 0.8,0.7 \n..."
prompt_eval_precision: "Tienes dos textos: un contexto y una referencia. Queremos evaluar si la referencia está bien representada en el contexto en cuanto a precisión. En una escala de 0.0 a 1.0, ¿qué tan bien se ajusta la referencia al contexto en términos de precisión (0.0 = no es relevante, 1.0 = completamente precisa)? Devuelve una puntuación en esta forma: 0.8"
prompt_eval_recall: "Tienes dos textos: un contexto y una referencia. Queremos evaluar si la referencia está bien representada en el contexto en cuanto a recall. En una escala de 0.0 a 1.0, ¿qué tan bien se ajusta la referencia al contexto en términos de recall (0.0 = no cubre la referencia, 1.0 = cubre completamente la referencia)? Devuelve una puntuación en esta forma: 0.7"
prompt_eval_faith: "Evalúa si la respuesta es fiel a la información dada en el contexto.Devuelve 'Sí' si la respuesta es completamente fiel al contexto y 'No' si hay alguna imprecisión o información incorrecta."
prompt_eval_factual_correctness: "Tienes dos textos: una respuesta y una referencia. Queremos evaluar si la respuesta está bien representada en términos de corrección factual en comparación con la referencia. En una escala de 0.0 a 1.0, ¿qué tan correcta es la respuesta en relación con la referencia en términos de hechos (0.0 = completamente incorrecta, 1.0 = completamente correcta)? Devuelve una puntuación en esta forma: 0.9"
prompt_eval_similarity: "Tienes dos textos: una respuesta y una referencia. Queremos evaluar cuánta similitud existe entre la respuesta y la referencia. En una escala de 0.0 a 1.0, ¿qué tan similar es la respuesta a la referencia en términos de contenido (0.0 = nada similar, 1.0 = completamente similar)? Devuelve una puntuación en esta forma: 0.85"